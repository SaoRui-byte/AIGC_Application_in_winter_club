import streamlit as st
from zhipuai import ZhipuAI
import os
import base64
import tempfile
from dotenv import load_dotenv
import sounddevice as sd
import soundfile as sf
import numpy as np
from aip import AipSpeech
import io
import re

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
client = ZhipuAI(api_key=os.getenv("ZHIPU_API_KEY"))

# ç™¾åº¦è¯­éŸ³é…ç½®
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_API_KEY = os.getenv("BAIDU_API_KEY")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")
baidu_client = AipSpeech(BAIDU_APP_ID, BAIDU_API_KEY, BAIDU_SECRET_KEY) if all([BAIDU_APP_ID, BAIDU_API_KEY, BAIDU_SECRET_KEY]) else None

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼ˆæ–°å¢æ–°å›¾ç‰‡ä¸Šä¼ æ ‡å¿—ï¼‰
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  
if "uploaded_image_base64" not in st.session_state:
    st.session_state.uploaded_image_base64 = None
if "tts_audio_segments" not in st.session_state:
    st.session_state.tts_audio_segments = []
if "last_image_uploaded" not in st.session_state:
    st.session_state.last_image_uploaded = None
# æ–°å¢ï¼šè·Ÿè¸ªæ˜¯å¦åˆšä¸Šä¼ äº†æ–°å›¾ç‰‡
if "is_new_image_uploaded" not in st.session_state:
    st.session_state.is_new_image_uploaded = False

# ------------------------------
# è¾…åŠ©å‡½æ•°ï¼šæ„å›¾æ£€æµ‹
# ------------------------------
def detect_intent(user_input):
    """æ£€æµ‹ç”¨æˆ·è¾“å…¥çš„æ„å›¾ï¼Œåˆ¤æ–­æ˜¯å›æº¯å†å²è¿˜æ˜¯å½“å‰å›¾ç‰‡æé—®"""
    # å›æº¯å†å²çš„å…³é”®è¯ï¼ˆæ–°å¢"ä¸Šä¸€ä¸ªé—®é¢˜"ç­‰ç²¾å‡†å…³é”®è¯ï¼‰
    history_keywords = ["ä¹‹å‰", "åˆšæ‰", "ä¹‹å‰é—®çš„", "é‚£åª", "ä¹‹å‰çš„", "ä¹‹å‰è¯´çš„", 
                       "ä¹‹å‰çš„é—®é¢˜", "ä¸Šä¸€ä¸ªé—®é¢˜", "ä¸Šä¸€ä¸ª", "åˆšæ‰é—®çš„"]
    # å½“å‰å›¾ç‰‡çš„å…³é”®è¯
    current_image_keywords = ["è¿™åª", "è¿™æ˜¯ä»€ä¹ˆ", "å®ƒ", "è¿™å¼ ", "å½“å‰", "ç°åœ¨"]
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›æº¯å†å²çš„å…³é”®è¯
    if any(keyword in user_input for keyword in history_keywords):
        return "history"
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å½“å‰å›¾ç‰‡çš„å…³é”®è¯
    elif any(keyword in user_input for keyword in current_image_keywords):
        return "current_image"
    else:
        return "default"

# ------------------------------
# 1. æœ¬åœ°å½•éŸ³åŠŸèƒ½
# ------------------------------
def record_audio_with_sounddevice(duration=5, samplerate=16000):
    try:
        st.info(f"ğŸ¤ å¼€å§‹å½•éŸ³ {duration} ç§’...è¯·å¯¹ç€éº¦å…‹é£è¯´è¯ï¼")
        audio_data = sd.rec(
            int(duration * samplerate),
            samplerate=samplerate,
            channels=1,
            dtype='int16'
        )
        sd.wait()
        
        # è½¬ä¸ºWAVå­—èŠ‚æµ
        wav_buffer = tempfile.SpooledTemporaryFile()
        sf.write(wav_buffer, audio_data, samplerate, format='WAV')
        wav_buffer.seek(0)
        wav_bytes = wav_buffer.read()
        wav_buffer.close()
        
        st.success("âœ… å½•éŸ³å®Œæˆï¼æ­£åœ¨è¯†åˆ«è¯­éŸ³å†…å®¹...")
        return wav_bytes
    except Exception as e:
        st.error(f"âŒ å½•éŸ³å¤±è´¥ï¼š{str(e)}")
        return None

# ------------------------------
# 2. ç™¾åº¦è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰
# ------------------------------
def baidu_speech_to_text(wav_bytes):
    if not baidu_client:
        st.error("âŒ æœªé…ç½®ç™¾åº¦è¯­éŸ³å‚æ•°ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
        return ""
    
    try:
        pcm_data = wav_bytes[44:] if len(wav_bytes) > 44 else wav_bytes
        result = baidu_client.asr(pcm_data, 'pcm', 16000, {'dev_pid': 1537})
        
        if result.get("err_no") == 0 and "result" in result and len(result["result"]) > 0:
            return result["result"][0]
        elif result.get("err_no") == 3301:
            st.warning("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆå£°éŸ³ï¼Œè¯·æé«˜éŸ³é‡")
        else:
            st.error(f"âŒ è¯†åˆ«å¤±è´¥ï¼š{result.get('err_msg', 'æœªçŸ¥é”™è¯¯')}")
        return ""
    except Exception as e:
        st.error(f"âŒ è°ƒç”¨ç™¾åº¦æ¥å£å‡ºé”™ï¼š{str(e)}")
        return ""

# ------------------------------
# 3. ç™¾åº¦è¯­éŸ³åˆæˆï¼ˆTTSï¼‰
# ------------------------------
def baidu_text_to_speech(text, per=0):
    if not baidu_client:
        st.error("âŒ æœªé…ç½®ç™¾åº¦è¯­éŸ³å‚æ•°ï¼Œæ— æ³•æ’­æŠ¥è¯­éŸ³")
        return None
    
    # æ–‡æœ¬æ¸…æ´—
    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    if not text:
        st.warning("âš ï¸ æ— æœ‰æ•ˆæ–‡æœ¬å¯åˆæˆè¯­éŸ³")
        return None
    
    MAX_SEGMENT_LEN = 500
    text_segments = [text[i:i+MAX_SEGMENT_LEN].strip() for i in range(0, len(text), MAX_SEGMENT_LEN) if text[i:i+MAX_SEGMENT_LEN].strip()]
    
    try:
        audio_segments = []
        for idx, segment in enumerate(text_segments):
            result = baidu_client.synthesis(
                segment,
                'zh',
                1,
                {
                    'vol': 5,
                    'per': per,
                    'spd': 5,
                    'pit': 5,
                    'aue': 3
                }
            )
            
            if isinstance(result, dict):
                st.error(f"âŒ ç¬¬{idx+1}æ®µåˆæˆå¤±è´¥ï¼š{result.get('err_msg', 'æœªçŸ¥é”™è¯¯')}")
                return None
            audio_segments.append(result)
        
        st.success(f"âœ… è¯­éŸ³åˆæˆå®Œæˆï¼ˆå…±{len(audio_segments)}æ®µï¼‰")
        return audio_segments
    except Exception as e:
        st.error(f"âŒ è¯­éŸ³åˆæˆå‡ºé”™ï¼š{str(e)}")
        return None

# ------------------------------
# 4. å‰ç«¯éŸ³é¢‘åˆå¹¶æ’­æ”¾
# ------------------------------
def merge_audio_frontend(audio_segments):
    if not audio_segments:
        return None
    
    segment_base64_list = [base64.b64encode(seg).decode('utf-8') for seg in audio_segments]
    
    js_code = f"""
    <script>
    let audioContext = null;
    let source = null;
    let mergedBuffer = null;
    let isPlaying = false;
    let startTime = 0;
    let pauseTime = 0;

    async function togglePlayback() {{
        if (!audioContext) {{
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const buffers = [];
            for (const base64 of {segment_base64_list}) {{
                const response = await fetch(`data:audio/mp3;base64,${{base64}}`);
                const arrayBuffer = await response.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                buffers.push(audioBuffer);
            }}
            const totalLength = buffers.reduce((sum, buf) => sum + buf.length, 0);
            mergedBuffer = audioContext.createBuffer(1, totalLength, buffers[0].sampleRate);
            let offset = 0;
            for (const buf of buffers) {{
                mergedBuffer.getChannelData(0).set(buf.getChannelData(0), offset);
                offset += buf.length;
            }}
        }}

        if (isPlaying) {{
            source.stop();
            pauseTime = audioContext.currentTime - startTime;
            isPlaying = false;
            document.getElementById('mergePlayBtn').innerText = 'â–¶ï¸ ç»§ç»­æ’­æ”¾å®Œæ•´è¯­éŸ³';
        }} else {{
            if (source && source.state === 'running') {{
                source.stop();
            }}
            source = audioContext.createBufferSource();
            source.buffer = mergedBuffer;
            source.connect(audioContext.destination);
            source.start(0, pauseTime);
            startTime = audioContext.currentTime - pauseTime;
            isPlaying = true;
            document.getElementById('mergePlayBtn').innerText = 'â¸ï¸ æš‚åœæ’­æ”¾';

            source.onended = () => {{
                isPlaying = false;
                pauseTime = 0;
                document.getElementById('mergePlayBtn').innerText = 'ğŸ§ æ’­æ”¾åˆå¹¶åçš„å®Œæ•´è¯­éŸ³';
            }};
        }}
    }}
    </script>
    <button id="mergePlayBtn" onclick="togglePlayback()" style="padding: 8px 16px; background: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer;">
    ğŸ§ æ’­æ”¾åˆå¹¶åçš„å®Œæ•´è¯­éŸ³
    </button>
    """
    return js_code

# ------------------------------
# 5. æ™ºè°±AIå¯¹è¯
# ------------------------------
def pet_multimodal_chat(image_base64, user_input, chat_history, use_history=True):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å® ç‰©ä¸“å®¶ï¼Œç²¾é€šåŠ¨ç‰©å“ç§å’ŒåŠ¨ç‰©åŒ»ç–—æ–¹é¢çŸ¥è¯†ï¼Œå›ç­”è¦ç®€æ´ç²¾å‡†ã€‚å¦‚æœç”¨æˆ·æé—®æ¶‰åŠå“ç§è¯†åˆ«ï¼Œè¯·å…ˆè¯†åˆ«å“ç§ï¼Œå†å›ç­”é—®é¢˜ï¼›å¦‚æœç”¨æˆ·åˆ¤æ–­é”™è¯¯ï¼Œè¦æŒ‡å‡ºå¹¶è§£é‡Šã€‚"}
    ]
    
    # æ ¹æ®use_historyå†³å®šæ˜¯å¦æ·»åŠ å†å²å¯¹è¯
    if use_history:
        for msg in chat_history:
            messages.append({"role": msg["role"], "content": msg["content"]})
    
    messages.append({
        "role": "user",
        "content": [
            {"type": "text", "text": user_input},
            {"type": "image_url", "image_url": {"url": image_base64}}
        ]
    })
    
    try:
        response = client.chat.completions.create(
            model="glm-4v",
            messages=messages,
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"âŒ å¤šæ¨¡æ€è¯·æ±‚å‡ºé”™ï¼š{str(e)}")
        return "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•å¤„ç†å›¾ç‰‡è¯·æ±‚ï¼Œè¯·ç¨åå†è¯•ã€‚"

# ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ–°å¢exclude_last_userå‚æ•°ï¼Œæ’é™¤å½“å‰æé—®ï¼Œåªä¼ æ›´æ—©çš„å†å²
def pet_text_chat(user_input, chat_history, use_history=True, exclude_last_user=False):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å® ç‰©å…»æŠ¤åŠ©æ‰‹ï¼Œç»“åˆå†å²å¯¹è¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå›ç­”è¦ä¸ªæ€§åŒ–ã€ç®€æ´å®ç”¨ã€‚å¦‚æœç”¨æˆ·é—®ä¸Šä¸€ä¸ªé—®é¢˜/ä¹‹å‰çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Œè¯·å‡†ç¡®å¼•ç”¨å†å²å¯¹è¯å†…å®¹å›ç­”ã€‚"}
    ]
    
    # æ ¹æ®use_historyå†³å®šæ˜¯å¦æ·»åŠ å†å²å¯¹è¯
    if use_history:
        # æ’é™¤æœ€åä¸€è½®ç”¨æˆ·æé—®ï¼ˆå½“å‰çš„å›æº¯æé—®ï¼‰ï¼Œåªä¼ æ›´æ—©çš„å†å²
        if exclude_last_user and len(chat_history) >= 2:
            history_to_use = chat_history[:-1]  # å»æ‰æœ€åä¸€æ¡ï¼ˆå½“å‰ç”¨æˆ·æé—®ï¼‰
        else:
            history_to_use = chat_history
        
        for msg in history_to_use:
            messages.append({"role": msg["role"], "content": msg["content"]})
    
    messages.append({"role": "user", "content": user_input})
    
    try:
        response = client.chat.completions.create(
            model="glm-4",
            messages=messages,
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"âŒ æ–‡æœ¬è¯·æ±‚å‡ºé”™ï¼š{str(e)}")
        return "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•å¤„ç†è¯·æ±‚ï¼Œè¯·ç¨åå†è¯•ã€‚"

# ------------------------------
# 6. ç•Œé¢å¸ƒå±€ï¼ˆæ ¸å¿ƒï¼šè‡ªåŠ¨åˆ‡æ¢é€»è¾‘ï¼‰
# ------------------------------
st.title("ğŸ¾ å® ç‰©è¯†åˆ«ä¸å…»æŠ¤åŠ©æ‰‹ ")

# ä¾§è¾¹æ 
with st.sidebar:
    # ç™¾åº¦è¯­éŸ³çŠ¶æ€
    if baidu_client:
        st.success("âœ… å·²è¿æ¥ç™¾åº¦è¯­éŸ³è¯†åˆ«/åˆæˆæœåŠ¡")
    else:
        st.error("âŒ æœªé…ç½®ç™¾åº¦è¯­éŸ³å‚æ•°")
    
    # å›¾ç‰‡ä¸Šä¼ 
    st.subheader("ğŸ“· ä¸Šä¼ å® ç‰©ç…§ç‰‡")
    uploaded_image = st.file_uploader(
        "é€‰æ‹©ç…§ç‰‡ï¼ˆjpg/pngï¼‰", 
        type=["jpg", "png", "jpeg"],
        key="pet_image_uploader",
        help="ä¸Šä¼ æ–°ç…§ç‰‡ä¼šè‡ªåŠ¨è§¦å‘ã€Œåªçœ‹å½“å‰ç…§ç‰‡ï¼Œä¸å‚è€ƒå†å²ã€æ¨¡å¼"
    )
    
    # æ£€æµ‹æ–°å›¾ç‰‡ä¸Šä¼ ï¼ˆæ ¸å¿ƒï¼šè®¾ç½®æ–°å›¾ç‰‡æ ‡å¿—ï¼‰
    if uploaded_image:
        image_identifier = f"{uploaded_image.name}_{uploaded_image.size}"
        if image_identifier != st.session_state.last_image_uploaded:
            image_base64 = base64.b64encode(uploaded_image.getvalue()).decode("utf-8")
            st.session_state.uploaded_image_base64 = f"data:image/jpeg;base64,{image_base64}"
            st.session_state.last_image_uploaded = image_identifier
            # å…³é”®ï¼šæ ‡è®°ä¸ºåˆšä¸Šä¼ æ–°å›¾ç‰‡
            st.session_state.is_new_image_uploaded = True
            st.success("âœ… æ–°å›¾ç‰‡å·²ä¸Šä¼ ï¼AIå°†ä»…å‚è€ƒå½“å‰ç…§ç‰‡å›ç­”ï¼Œä¸ä½¿ç”¨å†å²å¯¹è¯")
        st.image(uploaded_image, caption="å½“å‰ä¸Šä¼ çš„å® ç‰©ç…§ç‰‡", use_column_width=True)
    else:
        st.session_state.uploaded_image_base64 = None
        st.session_state.last_image_uploaded = None
        st.info("è¯·ä¸Šä¼ å® ç‰©ç…§ç‰‡ä»¥å¯ç”¨å›¾ç‰‡è¯†åˆ«åŠŸèƒ½")
    
    st.divider()
    
    # è¯­éŸ³è®¾ç½®
    st.subheader("ğŸ¤ æœ¬åœ°è¯­éŸ³æé—®")
    record_duration = st.number_input("å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰", min_value=1, max_value=10, value=5, step=1, key="record_duration")
    
    st.subheader("ğŸ”Š è¯­éŸ³æ’­æŠ¥è®¾ç½®")
    voice_type = st.selectbox(
        "é€‰æ‹©å‘éŸ³äºº",
        options=["å¥³å£°ï¼ˆé»˜è®¤ï¼‰", "ç”·å£°", "æƒ…æ„Ÿå¥³å£°", "æƒ…æ„Ÿç”·å£°"],
        index=0,
        key="voice_type"
    )
    per_map = {"å¥³å£°ï¼ˆé»˜è®¤ï¼‰":0, "ç”·å£°":1, "æƒ…æ„Ÿå¥³å£°":3, "æƒ…æ„Ÿç”·å£°":4}
    selected_per = per_map[voice_type]
    
    # å½•éŸ³æŒ‰é’®
    if st.button("â–¶ï¸ å¼€å§‹å½•éŸ³å¹¶è¯†åˆ«", type="primary", key="record_btn"):
        wav_bytes = record_audio_with_sounddevice(duration=record_duration)
        if not wav_bytes:
            st.stop()
        
        recognized_text = baidu_speech_to_text(wav_bytes)
        if not recognized_text:
            st.stop()
        
        st.success(f"âœ… è¯­éŸ³è¯†åˆ«ç»“æœï¼š{recognized_text}")
        user_prompt = recognized_text
        
        # æ·»åŠ ç”¨æˆ·è¯­éŸ³è¾“å…¥åˆ°å¯¹è¯å†å²
        with st.chat_message("user"):
            st.markdown(f"ğŸ¤ è¯­éŸ³è¾“å…¥ï¼š{user_prompt}")
        st.session_state.chat_history.append({"role": "user", "content": user_prompt})
        
        # è‡ªåŠ¨åˆ¤æ–­æ¨¡å¼
        intent = detect_intent(user_prompt)
        # åˆšä¸Šä¼ æ–°å›¾ç‰‡ â†’ åªçœ‹å›¾ç‰‡ï¼Œä¸çœ‹å†å²
        if st.session_state.is_new_image_uploaded:
            use_image = True
            use_history = False
            # é‡ç½®æ–°å›¾ç‰‡æ ‡å¿—
            st.session_state.is_new_image_uploaded = False
        elif intent == "history":
            # å›æº¯å†å² â†’ åªçœ‹å†å²ï¼Œä¸çœ‹å›¾ç‰‡
            use_image = False
            use_history = True
        elif intent == "current_image":
            # å½“å‰å›¾ç‰‡æé—® â†’ çœ‹å›¾ç‰‡+å†å²
            use_image = True
            use_history = True
        else:
            # é»˜è®¤æ¨¡å¼
            use_image = True if st.session_state.uploaded_image_base64 else False
            use_history = True
        
        # ç”ŸæˆAIå›å¤
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” æ­£åœ¨ç”Ÿæˆå›å¤..."):
                if use_image and st.session_state.uploaded_image_base64:
                    response = pet_multimodal_chat(st.session_state.uploaded_image_base64, user_prompt, st.session_state.chat_history, use_history)
                else:
                    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘å›æº¯å†å²æ—¶ï¼Œæ’é™¤å½“å‰æé—®
                    exclude_last = True if intent == "history" else False
                    response = pet_text_chat(user_prompt, st.session_state.chat_history, use_history, exclude_last)
            st.markdown(response)
            
            # è¯­éŸ³åˆæˆ
            tts_audio_segments = baidu_text_to_speech(response, per=selected_per)
            if tts_audio_segments:
                st.session_state.tts_audio_segments = tts_audio_segments
                merge_js = merge_audio_frontend(st.session_state.tts_audio_segments)
                if merge_js:
                    st.components.v1.html(merge_js, height=50)
                for idx, audio_bytes in enumerate(st.session_state.tts_audio_segments):
                    st.caption(f"ğŸ§ è¯­éŸ³æ’­æŠ¥ - ç¬¬{idx+1}æ®µ")
                    st.audio(audio_bytes, format='audio/mp3', start_time=0)
        
        # æ·»åŠ AIå›å¤åˆ°å¯¹è¯å†å²
        st.session_state.chat_history.append({"role": "assistant", "content": response})
    
    st.divider()
    
    # åŠŸèƒ½æŒ‰é’®
    if st.button("â¹ï¸ ç»“æŸé¡¹ç›®", type="primary", key="stop_btn"):
        st.warning("âš ï¸ é¡¹ç›®å·²åœæ­¢è¿è¡Œï¼")
        st.info("âœ… è¯·åœ¨ç»ˆç«¯æŒ‰ Ctrl + C å½»åº•å…³é—­æœåŠ¡")
        st.stop()
    
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²", key="clear_chat"):
        st.session_state.chat_history = []
        st.session_state.tts_audio_segments = []
        st.rerun()

# ------------------------------
# èŠå¤©ç•Œé¢
# ------------------------------
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        # ä¿®å¤ï¼šä½¿ç”¨å…¨å±€çŠ¶æ€çš„tts_audio_segmentsï¼Œè€Œéå±€éƒ¨å˜é‡
        if msg["role"] == "assistant" and st.session_state.tts_audio_segments:
            merge_js = merge_audio_frontend(st.session_state.tts_audio_segments)
            if merge_js:
                st.components.v1.html(merge_js, height=50)
            for seg_idx, audio_bytes in enumerate(st.session_state.tts_audio_segments):
                st.caption(f"ğŸ§ è¯­éŸ³æ’­æŠ¥ - ç¬¬{seg_idx+1}æ®µ")
                st.audio(audio_bytes, format='audio/mp3')

# æ–‡å­—è¾“å…¥æ¡†
user_prompt = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆå¦‚ï¼šå®ƒä¸€ç›´æŒ è€³æœµæ€ä¹ˆåŠï¼Ÿï¼‰", key="chat_input")
if user_prompt:
    # æ·»åŠ æ–‡å­—è¾“å…¥åˆ°å¯¹è¯å†å²
    with st.chat_message("user"):
        st.markdown(user_prompt)
    st.session_state.chat_history.append({"role": "user", "content": user_prompt})
    
    # è‡ªåŠ¨åˆ¤æ–­æ¨¡å¼
    intent = detect_intent(user_prompt)
    # åˆšä¸Šä¼ æ–°å›¾ç‰‡ â†’ åªçœ‹å›¾ç‰‡ï¼Œä¸çœ‹å†å²
    if st.session_state.is_new_image_uploaded:
        use_image = True
        use_history = False
        # é‡ç½®æ–°å›¾ç‰‡æ ‡å¿—
        st.session_state.is_new_image_uploaded = False
    elif intent == "history":
        # å›æº¯å†å² â†’ åªçœ‹å†å²ï¼Œä¸çœ‹å›¾ç‰‡
        use_image = False
        use_history = True
    elif intent == "current_image":
        # å½“å‰å›¾ç‰‡æé—® â†’ çœ‹å›¾ç‰‡+å†å²
        use_image = True
        use_history = True
    else:
        # é»˜è®¤æ¨¡å¼
        use_image = True if st.session_state.uploaded_image_base64 else False
        use_history = True
    
    # ç”ŸæˆAIå›å¤
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ€è€ƒå›å¤..."):
            if use_image and st.session_state.uploaded_image_base64:
                response = pet_multimodal_chat(st.session_state.uploaded_image_base64, user_prompt, st.session_state.chat_history, use_history)
            else:
                # ã€æ ¸å¿ƒä¿®æ”¹ã€‘å›æº¯å†å²æ—¶ï¼Œä¼ å…¥exclude_last_user=Trueï¼Œæ’é™¤å½“å‰æé—®
                exclude_last = True if intent == "history" else False
                response = pet_text_chat(user_prompt, st.session_state.chat_history, use_history, exclude_last)
        st.markdown(response)
        
        # è¯­éŸ³åˆæˆ
        selected_per = per_map.get(st.session_state.get("voice_type", "å¥³å£°ï¼ˆé»˜è®¤ï¼‰"), 0)
        tts_audio_segments = baidu_text_to_speech(response, per=selected_per)
        if tts_audio_segments:
            st.session_state.tts_audio_segments = tts_audio_segments
            merge_js = merge_audio_frontend(st.session_state.tts_audio_segments)
            if merge_js:
                st.components.v1.html(merge_js, height=50)
            for idx, audio_bytes in enumerate(st.session_state.tts_audio_segments):
                st.caption(f"ğŸ§ è¯­éŸ³æ’­æŠ¥ - ç¬¬{idx+1}æ®µ")
                st.audio(audio_bytes, format='audio/mp3', start_time=0)
    
    # æ·»åŠ AIå›å¤åˆ°å¯¹è¯å†å²
    st.session_state.chat_history.append({"role": "assistant", "content": response})