import streamlit as st
from zhipuai import ZhipuAI
import os
import base64
import tempfile
from dotenv import load_dotenv
import sounddevice as sd
import soundfile as sf
import numpy as np
from aip import AipSpeech  # ç™¾åº¦è¯­éŸ³è¯†åˆ«/åˆæˆSDK
import io
import re  # æ–‡æœ¬æ¸…æ´—ç”¨

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
client = ZhipuAI(api_key=os.getenv("ZHIPU_API_KEY"))

# ç™¾åº¦è¯­éŸ³é…ç½®ï¼ˆè¯†åˆ«+åˆæˆå…±ç”¨ï¼‰
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_API_KEY = os.getenv("BAIDU_API_KEY")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")
baidu_client = AipSpeech(BAIDU_APP_ID, BAIDU_API_KEY, BAIDU_SECRET_KEY) if BAIDU_APP_ID and BAIDU_API_KEY and BAIDU_SECRET_KEY else None

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼ˆæ–°å¢å›¾ç‰‡ä¸Šä¼ çš„çŠ¶æ€è·Ÿè¸ªï¼‰
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # å¯¹è¯å†å²
if "uploaded_image_base64" not in st.session_state:
    st.session_state.uploaded_image_base64 = None  # ä¸Šä¼ çš„å›¾ç‰‡
if "tts_audio_segments" not in st.session_state:
    st.session_state.tts_audio_segments = []  # å­˜å‚¨åˆ†æ®µéŸ³é¢‘å­—èŠ‚æµ
if "last_image_uploaded" not in st.session_state:
    st.session_state.last_image_uploaded = None  # è·Ÿè¸ªæœ€åä¸€æ¬¡ä¸Šä¼ çš„å›¾ç‰‡æ ‡è¯†

# ------------------------------
# æ ¸å¿ƒ1ï¼šsounddeviceæœ¬åœ°å½•éŸ³
# ------------------------------
def record_audio_with_sounddevice(duration=5, samplerate=16000):
    try:
        st.info(f"ğŸ¤ å¼€å§‹å½•éŸ³ {duration} ç§’...è¯·å¯¹ç€éº¦å…‹é£è¯´è¯ï¼")
        audio_data = sd.rec(
            int(duration * samplerate),
            samplerate=samplerate,
            channels=1,
            dtype='int16'
        )
        sd.wait()
        
        # è½¬ä¸ºWAVå­—èŠ‚æµ
        wav_buffer = tempfile.SpooledTemporaryFile()
        sf.write(wav_buffer, audio_data, samplerate, format='WAV')
        wav_buffer.seek(0)
        wav_bytes = wav_buffer.read()
        wav_buffer.close()
        
        st.success("âœ… å½•éŸ³å®Œæˆï¼æ­£åœ¨è¯†åˆ«è¯­éŸ³å†…å®¹...")
        return wav_bytes
    except Exception as e:
        st.error(f"âŒ å½•éŸ³å¤±è´¥ï¼š{str(e)}")
        return None

# ------------------------------
# æ ¸å¿ƒ2ï¼šç™¾åº¦è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰
# ------------------------------
def baidu_speech_to_text(wav_bytes):
    if not baidu_client:
        st.error("âŒ æœªé…ç½®ç™¾åº¦è¯­éŸ³å‚æ•°ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
        return ""
    
    try:
        pcm_data = wav_bytes[44:] if len(wav_bytes) > 44 else wav_bytes
        result = baidu_client.asr(pcm_data, 'pcm', 16000, {'dev_pid': 1537})
        
        if result.get("err_no") == 0 and "result" in result and len(result["result"]) > 0:
            return result["result"][0]
        elif result.get("err_no") == 3301:
            st.warning("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆå£°éŸ³ï¼Œè¯·æé«˜éŸ³é‡")
        else:
            st.error(f"âŒ è¯†åˆ«å¤±è´¥ï¼š{result.get('err_msg', 'æœªçŸ¥é”™è¯¯')}")
        return ""
    except Exception as e:
        st.error(f"âŒ è°ƒç”¨ç™¾åº¦æ¥å£å‡ºé”™ï¼š{str(e)}")
        return ""

# ------------------------------
# æ ¸å¿ƒ3ï¼šç™¾åº¦è¯­éŸ³åˆæˆï¼ˆTTSï¼‰- ä¿®å¤æ‰€æœ‰æŠ¥é”™
# ------------------------------
def baidu_text_to_speech(text, per=0):
    """
    ç™¾åº¦æ–‡å­—è½¬è¯­éŸ³ï¼ˆTTSï¼‰- æ— ffmpeg/pydub + ä¿®å¤param err + å…¼å®¹æ—§ç‰ˆStreamlit
    """
    if not baidu_client:
        st.error("âŒ æœªé…ç½®ç™¾åº¦è¯­éŸ³å‚æ•°ï¼Œæ— æ³•æ’­æŠ¥è¯­éŸ³")
        return None
    
    # 1. æ–‡æœ¬æ¸…æ´—ï¼ˆå»é™¤ç‰¹æ®Šå­—ç¬¦/æ¢è¡Œ/å¤šä½™ç©ºæ ¼ï¼‰
    text = re.sub(r'\n+', ' ', text)  # æ¢è¡Œæ›¿æ¢ä¸ºç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)  # å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
    text = text.strip()               # å»é™¤é¦–å°¾ç©ºæ ¼
    
    # 2. ç©ºæ–‡æœ¬æ ¡éªŒ
    MAX_SEGMENT_LEN = 500
    if not text or len(text) == 0:
        st.warning("âš ï¸ æ— æœ‰æ•ˆæ–‡æœ¬å¯åˆæˆè¯­éŸ³")
        return None
    
    try:
        # 3. æ–‡æœ¬åˆ†æ®µï¼ˆ500å­—/æ®µï¼‰
        text_segments = []
        if len(text) <= MAX_SEGMENT_LEN:
            text_segments = [text]
        else:
            st.warning(f"âš ï¸ æ–‡æœ¬è¿‡é•¿ï¼ˆ{len(text)}å­—ï¼‰ï¼Œå°†åˆ†ä¸º{len(text)//MAX_SEGMENT_LEN + 1}æ®µæ’­æ”¾")
            for i in range(0, len(text), MAX_SEGMENT_LEN):
                segment = text[i:i+MAX_SEGMENT_LEN].strip()
                if segment:  # è·³è¿‡ç©ºåˆ†æ®µ
                    text_segments.append(segment)
        
        # 4. é€æ®µåˆæˆè¯­éŸ³ï¼ˆä¿®æ­£ç™¾åº¦APIå‚æ•°é¡ºåºï¼‰
        audio_segments = []
        for idx, segment in enumerate(text_segments):
            # ç™¾åº¦TTSæ­£ç¡®å‚æ•°æ ¼å¼
            result = baidu_client.synthesis(
                segment,          # å‚æ•°1ï¼šè¦åˆæˆçš„æ–‡æœ¬
                'zh',             # å‚æ•°2ï¼šè¯­è¨€ï¼ˆä¸­æ–‡ï¼‰
                1,                # å‚æ•°3ï¼šå®¢æˆ·ç«¯ç±»å‹ï¼ˆå›ºå®š1ï¼‰
                {
                    'vol': 5,     # éŸ³é‡ï¼ˆ0-15ï¼‰
                    'per': per,   # å‘éŸ³äººï¼ˆ0=å¥³å£°ï¼Œ1=ç”·å£°ï¼Œ3=æƒ…æ„Ÿå¥³å£°ï¼Œ4=æƒ…æ„Ÿç”·å£°ï¼‰
                    'spd': 5,     # è¯­é€Ÿï¼ˆ0-9ï¼‰
                    'pit': 5,     # éŸ³è°ƒï¼ˆ0-9ï¼‰
                    'aue': 3      # éŸ³é¢‘æ ¼å¼ï¼ˆ3=mp3ï¼Œå…¼å®¹å‰ç«¯æ’­æ”¾ï¼‰
                }
            )
            
            # 5. å¤„ç†åˆæˆç»“æœ
            if isinstance(result, dict):
                st.error(f"âŒ ç¬¬{idx+1}æ®µåˆæˆå¤±è´¥ï¼š{result.get('err_msg', 'æœªçŸ¥é”™è¯¯')}")
                return None
            audio_segments.append(result)
            st.info(f"âœ… ç¬¬{idx+1}æ®µè¯­éŸ³åˆæˆå®Œæˆ")
        
        st.success("âœ… æ‰€æœ‰è¯­éŸ³æ®µåˆæˆå®Œæˆï¼å¯ä¾æ¬¡æ’­æ”¾æˆ–åˆå¹¶æ’­æ”¾")
        return audio_segments
    
    except Exception as e:
        st.error(f"âŒ è¯­éŸ³åˆæˆå‡ºé”™ï¼š{str(e)}")
        return None

# ------------------------------
# æ ¸å¿ƒ4ï¼šå‰ç«¯åˆå¹¶éŸ³é¢‘ï¼ˆWeb Audio APIï¼‰
# ------------------------------
def merge_audio_frontend(audio_segments):
    """
    å°†åˆ†æ®µéŸ³é¢‘å­—èŠ‚æµè½¬ä¸ºbase64ï¼Œä¼ ç»™å‰ç«¯ç”¨Web Audio APIåˆå¹¶ï¼ˆå¸¦æš‚åœ/é˜²é‡å åŠŸèƒ½ï¼‰
    """
    if not audio_segments or len(audio_segments) == 0:
        return None
    
    # å°†æ¯ä¸ªéŸ³é¢‘å­—èŠ‚æµè½¬ä¸ºbase64
    segment_base64_list = [base64.b64encode(seg).decode('utf-8') for seg in audio_segments]
    
    # ç”Ÿæˆå‰ç«¯åˆå¹¶éŸ³é¢‘çš„JavaScriptä»£ç ï¼ˆä¿®å¤é‡å +æš‚åœåŠŸèƒ½ï¼‰
    js_code = f"""
    <script>
    // å…¨å±€å˜é‡ç®¡ç†æ’­æ”¾çŠ¶æ€
    let audioContext = null;
    let source = null;
    let mergedBuffer = null;
    let isPlaying = false;
    let startTime = 0;
    let pauseTime = 0;

    async function togglePlayback() {{
        if (!audioContext) {{
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            // é¦–æ¬¡åŠ è½½æ—¶åˆå¹¶éŸ³é¢‘
            const buffers = [];
            for (const base64 of {segment_base64_list}) {{
                const response = await fetch(`data:audio/mp3;base64,${{base64}}`);
                const arrayBuffer = await response.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                buffers.push(audioBuffer);
            }}
            // åˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
            const totalLength = buffers.reduce((sum, buf) => sum + buf.length, 0);
            mergedBuffer = audioContext.createBuffer(1, totalLength, buffers[0].sampleRate);
            let offset = 0;
            for (const buf of buffers) {{
                mergedBuffer.getChannelData(0).set(buf.getChannelData(0), offset);
                offset += buf.length;
            }}
        }}

        if (isPlaying) {{
            // æš‚åœæ’­æ”¾
            source.stop();
            pauseTime = audioContext.currentTime - startTime;
            isPlaying = false;
            document.getElementById('mergePlayBtn').innerText = 'â–¶ï¸ ç»§ç»­æ’­æ”¾å®Œæ•´è¯­éŸ³';
        }} else {{
            // å¼€å§‹/ç»§ç»­æ’­æ”¾
            if (source && source.state === 'running') {{
                source.stop();
            }}
            source = audioContext.createBufferSource();
            source.buffer = mergedBuffer;
            source.connect(audioContext.destination);
            source.start(0, pauseTime);
            startTime = audioContext.currentTime - pauseTime;
            isPlaying = true;
            document.getElementById('mergePlayBtn').innerText = 'â¸ï¸ æš‚åœæ’­æ”¾';

            // æ’­æ”¾ç»“æŸåé‡ç½®çŠ¶æ€
            source.onended = () => {{
                isPlaying = false;
                pauseTime = 0;
                document.getElementById('mergePlayBtn').innerText = 'ğŸ§ æ’­æ”¾åˆå¹¶åçš„å®Œæ•´è¯­éŸ³';
            }};
        }}
    }}
    </script>
    <button id="mergePlayBtn" onclick="togglePlayback()" style="padding: 8px 16px; background: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer;">
    ğŸ§ æ’­æ”¾åˆå¹¶åçš„å®Œæ•´è¯­éŸ³
    </button>
    """
    return js_code

# ------------------------------
# æ ¸å¿ƒ5ï¼šæ™ºè°±AIå¯¹è¯/å¤šæ¨¡æ€è¯†åˆ«
# ------------------------------
def pet_multimodal_chat(image_base64, user_input, chat_history):
    context = "\n".join([f"{item['role']}: {item['content']}" for item in chat_history])
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": f"""
                    ä½ æ˜¯ä¸“ä¸šçš„å® ç‰©åŒ»ç”ŸåŠ©æ‰‹ï¼Œå›ç­”ç®€æ´ç²¾å‡†ï¼š
                    å†å²å¯¹è¯ï¼š{context}
                    ç”¨æˆ·é—®é¢˜ï¼š{user_input}
                    è¯·å…ˆè¯†åˆ«å“ç§ï¼Œå†åˆ†æå¥åº·çŠ¶æ€ï¼Œæœ€åç»™ä¸ªæ€§åŒ–å…»æŠ¤å»ºè®®ã€‚
                """},
                {"type": "image_url", "image_url": {"url": image_base64}}
            ]
        }
    ]
    try:
        response = client.chat.completions.create(model="glm-4v", messages=messages, temperature=0.3)
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"âŒ å¤šæ¨¡æ€è¯·æ±‚å‡ºé”™ï¼š{str(e)}")
        return "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•å¤„ç†å›¾ç‰‡è¯·æ±‚ï¼Œè¯·ç¨åå†è¯•ã€‚"

def pet_text_chat(user_input, chat_history):
    context = "\n".join([f"{item['role']}: {item['content']}" for item in chat_history])
    prompt = f"""
        ä½ æ˜¯ä¸“ä¸šçš„å® ç‰©å…»æŠ¤åŠ©æ‰‹ï¼Œç»“åˆå†å²å¯¹è¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¦ä¸ªæ€§åŒ–ï¼š
        å†å²å¯¹è¯ï¼š{context}
        ç”¨æˆ·é—®é¢˜ï¼š{user_input}
    """
    try:
        response = client.chat.completions.create(model="glm-4", messages=[{"role": "user", "content": prompt}], temperature=0.3)
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"âŒ æ–‡æœ¬è¯·æ±‚å‡ºé”™ï¼š{str(e)}")
        return "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•å¤„ç†è¯·æ±‚ï¼Œè¯·ç¨åå†è¯•ã€‚"

# ------------------------------
# ç•Œé¢å¸ƒå±€ï¼ˆä¿®å¤å›¾ç‰‡ä¸Šä¼ bugï¼‰
# ------------------------------
st.title("ğŸ¾ å® ç‰©è¯†åˆ«ä¸å…»æŠ¤åŠ©æ‰‹ | è¯­éŸ³äº¤äº’ç‰ˆ")
st.caption("ï¼ˆå¤§äºŒä½œä¸š Â· ç™¾åº¦è¯­éŸ³è¯†åˆ«+åˆæˆ + æ™ºè°±AI | æ— ffmpegä¾èµ–ï¼‰")

# ä¾§è¾¹æ åŠŸèƒ½åŒº
with st.sidebar:
    # ç™¾åº¦è¯­éŸ³çŠ¶æ€
    if baidu_client:
        st.success("âœ… å·²è¿æ¥ç™¾åº¦è¯­éŸ³è¯†åˆ«/åˆæˆæœåŠ¡")
    else:
        st.error("âŒ æœªé…ç½®ç™¾åº¦è¯­éŸ³å‚æ•°")
    
    # å›¾ç‰‡ä¸Šä¼ ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šæ·»åŠ key + å¼ºåˆ¶æ›´æ–°çŠ¶æ€ï¼‰
    st.subheader("ğŸ“· ä¸Šä¼ å® ç‰©ç…§ç‰‡")
    uploaded_image = st.file_uploader(
        "é€‰æ‹©ç…§ç‰‡ï¼ˆjpg/pngï¼‰", 
        type=["jpg", "png", "jpeg"],
        key="pet_image_uploader",  # å…³é”®ï¼šæ·»åŠ å”¯ä¸€keyï¼Œç¡®ä¿ç»„ä»¶çŠ¶æ€è·Ÿè¸ª
        help="ä¸Šä¼ æ–°å›¾ç‰‡ä¼šè‡ªåŠ¨æ›¿æ¢æ—§å›¾ç‰‡ï¼Œæ— éœ€æ¸…ç©ºå¯¹è¯"
    )
    
    # ä¿®å¤ï¼šæ£€æµ‹æ–°å›¾ç‰‡ä¸Šä¼ å¹¶å¼ºåˆ¶æ›´æ–°session_state
    if uploaded_image:
        # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ï¼ˆæ–‡ä»¶å+å¤§å°ï¼‰ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯æ–°å›¾ç‰‡
        image_identifier = f"{uploaded_image.name}_{uploaded_image.size}"
        if image_identifier != st.session_state.last_image_uploaded:
            image_base64 = base64.b64encode(uploaded_image.getvalue()).decode("utf-8")
            st.session_state.uploaded_image_base64 = f"data:image/jpeg;base64,{image_base64}"
            st.session_state.last_image_uploaded = image_identifier  # æ›´æ–°æœ€åä¸Šä¼ çš„æ ‡è¯†
            st.success("âœ… æ–°å›¾ç‰‡å·²ä¸Šä¼ å¹¶ç”Ÿæ•ˆï¼")
        st.image(uploaded_image, caption="å½“å‰ä¸Šä¼ çš„å® ç‰©ç…§ç‰‡", use_column_width=True)
    else:
        # æ— å›¾ç‰‡æ—¶é‡ç½®çŠ¶æ€
        st.session_state.uploaded_image_base64 = None
        st.session_state.last_image_uploaded = None
        st.info("è¯·ä¸Šä¼ å® ç‰©ç…§ç‰‡ä»¥å¯ç”¨å›¾ç‰‡è¯†åˆ«åŠŸèƒ½")
    
    # æ–°å¢ï¼šæ¸…ç©ºå›¾ç‰‡æŒ‰é’®ï¼ˆå¯é€‰ï¼‰
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå½“å‰å›¾ç‰‡", key="clear_image"):
        st.session_state.uploaded_image_base64 = None
        st.session_state.last_image_uploaded = None
        st.rerun()  # åˆ·æ–°ç•Œé¢
    
    st.divider()
    
    # æœ¬åœ°å½•éŸ³æ¨¡å—
    st.subheader("ğŸ¤ æœ¬åœ°è¯­éŸ³æé—®")
    record_duration = st.number_input("å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰", min_value=1, max_value=10, value=5, step=1, key="record_duration")
    
    # è¯­éŸ³æ’­æŠ¥å‘éŸ³äººé€‰æ‹©
    st.subheader("ğŸ”Š è¯­éŸ³æ’­æŠ¥è®¾ç½®")
    voice_type = st.selectbox(
        "é€‰æ‹©å‘éŸ³äºº",
        options=["å¥³å£°ï¼ˆé»˜è®¤ï¼‰", "ç”·å£°", "æƒ…æ„Ÿå¥³å£°", "æƒ…æ„Ÿç”·å£°"],
        index=0,
        key="voice_type",
        help="ä¸åŒå‘éŸ³äººæ•ˆæœä¸åŒï¼Œå¯æŒ‰éœ€é€‰æ‹©"
    )
    per_map = {"å¥³å£°ï¼ˆé»˜è®¤ï¼‰":0, "ç”·å£°":1, "æƒ…æ„Ÿå¥³å£°":3, "æƒ…æ„Ÿç”·å£°":4}
    selected_per = per_map[voice_type]
    
    # å¼€å§‹å½•éŸ³æŒ‰é’®
    if st.button("â–¶ï¸ å¼€å§‹å½•éŸ³å¹¶è¯†åˆ«", type="primary", key="record_btn"):
        # å½•éŸ³ â†’ è¯†åˆ« â†’ å¯¹è¯ â†’ åˆæˆè¯­éŸ³
        wav_bytes = record_audio_with_sounddevice(duration=record_duration)
        if not wav_bytes:
            st.stop()
        
        recognized_text = baidu_speech_to_text(wav_bytes)
        if not recognized_text:
            st.stop()
        
        st.success(f"âœ… è¯­éŸ³è¯†åˆ«ç»“æœï¼š{recognized_text}")
        user_prompt = recognized_text
        
        # å±•ç¤ºç”¨æˆ·è¾“å…¥ï¼ˆä¿®å¤ï¼šå»æ‰keyå‚æ•°ï¼‰
        with st.chat_message("user"):
            st.markdown(user_prompt)
        st.session_state.chat_history.append({"role": "user", "content": user_prompt})
        
        # è°ƒç”¨æ™ºè°±AIç”Ÿæˆå›å¤ï¼ˆä½¿ç”¨æœ€æ–°çš„å›¾ç‰‡ï¼‰
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” æ­£åœ¨ç”Ÿæˆå›å¤..."):
                if st.session_state.uploaded_image_base64:
                    response = pet_multimodal_chat(st.session_state.uploaded_image_base64, user_prompt, st.session_state.chat_history)
                else:
                    response = pet_text_chat(user_prompt, st.session_state.chat_history)
            st.markdown(response)
            
            # ç”Ÿæˆè¯­éŸ³ï¼ˆä¿®å¤åï¼‰
            tts_audio_segments = baidu_text_to_speech(response, per=selected_per)
            if tts_audio_segments:
                st.session_state.tts_audio_segments = tts_audio_segments
                # ç”Ÿæˆå‰ç«¯åˆå¹¶æ’­æ”¾çš„æŒ‰é’®
                merge_js = merge_audio_frontend(tts_audio_segments)
                if merge_js:
                    st.components.v1.html(merge_js, height=50)
                # ä¿ç•™åˆ†æ®µæ’­æ”¾æŒ‰é’®
                for idx, audio_bytes in enumerate(tts_audio_segments):
                    st.caption(f"ğŸ§ è¯­éŸ³æ’­æŠ¥ - ç¬¬{idx+1}æ®µ")
                    st.audio(audio_bytes, format='audio/mp3', start_time=0)
        
        st.session_state.chat_history.append({"role": "assistant", "content": response})
    
    st.divider()
    
    # åŠŸèƒ½æŒ‰é’®
    if st.button("â¹ï¸ ç»“æŸé¡¹ç›®", type="primary", key="stop_btn"):
        st.warning("âš ï¸ é¡¹ç›®å·²åœæ­¢è¿è¡Œï¼")
        st.info("âœ… è¯·åœ¨ç»ˆç«¯æŒ‰ Ctrl + C å½»åº•å…³é—­æœåŠ¡")
        st.stop()
    
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²", key="clear_chat"):
        st.session_state.chat_history = []
        st.session_state.tts_audio_segments = []
        # ä¿ç•™å›¾ç‰‡çŠ¶æ€ï¼ˆå¯é€‰ï¼šå¦‚éœ€æ¸…ç©ºå›¾ç‰‡ï¼Œå–æ¶ˆä¸‹é¢æ³¨é‡Šï¼‰
        # st.session_state.uploaded_image_base64 = None
        # st.session_state.last_image_uploaded = None
        st.rerun()

# ------------------------------
# èŠå¤©ç•Œé¢ï¼ˆä¿®å¤ï¼šå»æ‰æ‰€æœ‰st.chat_messageçš„keyå‚æ•°ï¼‰
# ------------------------------
# æ¸²æŸ“å†å²å¯¹è¯
for idx, msg in enumerate(st.session_state.chat_history):
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        # åŠ©æ‰‹å›å¤å±•ç¤ºè¯­éŸ³æ’­æ”¾æŒ‰é’®
        if msg["role"] == "assistant" and st.session_state.tts_audio_segments:
            # ç”Ÿæˆå‰ç«¯åˆå¹¶æ’­æ”¾çš„æŒ‰é’®
            merge_js = merge_audio_frontend(st.session_state.tts_audio_segments)
            if merge_js:
                st.components.v1.html(merge_js, height=50)
            # ä¿ç•™åˆ†æ®µæ’­æ”¾æŒ‰é’®
            for seg_idx, audio_bytes in enumerate(st.session_state.tts_audio_segments):
                st.caption(f"ğŸ§ è¯­éŸ³æ’­æŠ¥ - ç¬¬{seg_idx+1}æ®µ")
                st.audio(audio_bytes, format='audio/mp3')

# æ–‡å­—è¾“å…¥æ¡†ï¼ˆæ·»åŠ keyï¼‰
user_prompt = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆå¦‚ï¼šå®ƒä¸€ç›´æŒ è€³æœµæ€ä¹ˆåŠï¼Ÿï¼‰", key="chat_input")
if user_prompt:
    with st.chat_message("user"):
        st.markdown(user_prompt)
    st.session_state.chat_history.append({"role": "user", "content": user_prompt})
    
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ€è€ƒå›å¤..."):
            if st.session_state.uploaded_image_base64:
                response = pet_multimodal_chat(st.session_state.uploaded_image_base64, user_prompt, st.session_state.chat_history)
            else:
                response = pet_text_chat(user_prompt, st.session_state.chat_history)
        st.markdown(response)
        
        # ç”Ÿæˆè¯­éŸ³ï¼ˆä¿®å¤åï¼‰
        tts_audio_segments = baidu_text_to_speech(response, per=selected_per if 'selected_per' in locals() else 0)
        if tts_audio_segments:
            st.session_state.tts_audio_segments = tts_audio_segments
            # ç”Ÿæˆå‰ç«¯åˆå¹¶æ’­æ”¾çš„æŒ‰é’®
            merge_js = merge_audio_frontend(tts_audio_segments)
            if merge_js:
                st.components.v1.html(merge_js, height=50)
            # ä¿ç•™åˆ†æ®µæ’­æ”¾æŒ‰é’®
            for idx, audio_bytes in enumerate(tts_audio_segments):
                st.caption(f"ğŸ§ è¯­éŸ³æ’­æŠ¥ - ç¬¬{idx+1}æ®µ")
                st.audio(audio_bytes, format='audio/mp3', start_time=0)
    
    st.session_state.chat_history.append({"role": "assistant", "content": response})